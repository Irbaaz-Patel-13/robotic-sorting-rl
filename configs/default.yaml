# Default Configuration for Robotic Pick-and-Place Training
# =========================================================
#
# These hyperparameters were tuned through extensive experimentation.
# Key findings:
# - gamma=0.95 works better than 0.99 for manipulation tasks
# - Large batch sizes help with HER's goal relabeling
# - TQC outperforms SAC by about 20-30% sample efficiency

# Environment settings
env:
  id: "PandaPickAndPlace-v3"
  reward_type: "sparse"  # Use "dense" if not using HER
  max_episode_steps: 50

# Algorithm settings
algorithm:
  name: "TQC"  # Or "SAC" if sb3-contrib not installed
  use_her: true
  
  # Replay buffer
  buffer_size: 1000000
  
  # HER settings (only used if use_her=true)
  her:
    n_sampled_goal: 4
    goal_selection_strategy: "future"

# Hyperparameters (these are the tuned values that work well)
hyperparameters:
  learning_rate: 0.001
  batch_size: 512
  gamma: 0.95          # Lower than default, better for manipulation
  tau: 0.05            # Soft update coefficient
  learning_starts: 1000
  train_freq: 1
  gradient_steps: 1
  
  # Network architecture
  net_arch: [256, 256, 256]
  
  # TQC-specific
  n_critics: 2
  n_quantiles: 25
  top_quantiles_to_drop: 2

# Training settings
training:
  total_timesteps: 1000000
  eval_freq: 25000
  n_eval_episodes: 20
  checkpoint_freq: 50000
  log_freq: 5000

# Expected results with these settings:
# - 100k steps:  ~10-20% success
# - 300k steps:  ~30-40% success  
# - 500k steps:  ~50-60% success
# - 700k steps:  ~70-80% success
# - 850k steps:  ~85-95% success
# - 1M steps:    ~95-100% success
